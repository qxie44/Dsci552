{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Xie_Qinghui_HW3</h1></center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Qinghui Xie\n",
    "<br>\n",
    "Github Username: qxie44\n",
    "<br>\n",
    "USC ID: 7956658480"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Time Series Classification Part 1: Feature Creation/Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the AReM Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToData = '../data/AReM'\n",
    "#Bending1 - 7ds, Bending2 - 7ds, others- 15ds\n",
    "activity = ['bending1', 'bending2', 'cycling', 'lying', 'sitting', 'standing', 'walking']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Test and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Columns: time</th>\n",
       "      <th>avg_rss12</th>\n",
       "      <th>var_rss12</th>\n",
       "      <th>avg_rss13</th>\n",
       "      <th>var_rss13</th>\n",
       "      <th>avg_rss23</th>\n",
       "      <th>var_rss23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>32.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250</td>\n",
       "      <td>32.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>32.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>750</td>\n",
       "      <td>32.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>32.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>17.50</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1250</td>\n",
       "      <td>32.67</td>\n",
       "      <td>0.47</td>\n",
       "      <td>11.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>16.75</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1500</td>\n",
       "      <td>32.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.83</td>\n",
       "      <td>18.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1750</td>\n",
       "      <td>32.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.87</td>\n",
       "      <td>18.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000</td>\n",
       "      <td>32.33</td>\n",
       "      <td>0.47</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.33</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2250</td>\n",
       "      <td>32.67</td>\n",
       "      <td>0.47</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.82</td>\n",
       "      <td>18.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   # Columns: time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  \\\n",
       "0                0      32.50       0.50       0.00       0.00      19.00   \n",
       "1              250      32.50       0.50       0.00       0.00      18.50   \n",
       "2              500      32.75       0.43       1.00       0.00      18.00   \n",
       "3              750      32.50       0.50       0.00       0.00      17.50   \n",
       "4             1000      32.50       0.50       7.50       0.50      17.50   \n",
       "5             1250      32.67       0.47      11.00       1.00      16.75   \n",
       "6             1500      32.50       0.50       6.25       0.83      18.00   \n",
       "7             1750      32.50       0.50       3.50       0.87      18.00   \n",
       "8             2000      32.33       0.47       6.00       0.00      18.33   \n",
       "9             2250      32.67       0.47       8.00       0.82      18.00   \n",
       "\n",
       "   var_rss23  \n",
       "0       1.00  \n",
       "1       0.50  \n",
       "2       0.00  \n",
       "3       0.50  \n",
       "4       0.87  \n",
       "5       0.83  \n",
       "6       0.00  \n",
       "7       0.00  \n",
       "8       0.47  \n",
       "9       0.00  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = pd.read_csv('../data/AReM/bending2/dataset4.csv', delim_whitespace=True, skiprows=5, skipfooter=1, engine='python', header=None)\n",
    "df4.columns = ['# Columns: time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
    "df4.head(10)\n",
    "#data.to_csv('cleaned_bending2_data.csv', index=False)\n",
    "#chatgpt: how do I replace this newly cleaned file into the existing dataset4 file when going through the loop below:\n",
    "#\"if i == 'bending2':\n",
    "#                # Use the cleaned df4 DataFrame instead of reading from the file\n",
    "#                testData.append(df4)\n",
    "#            else:\n",
    "#                # Load test data and add a label\n",
    "#                df = pd.read_csv(file_path, skiprows=4)\n",
    "#                df['label'] = i  # Use the activity name as the label\n",
    "#               testData.append(df)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test: for bending keep 1-2, the rest 1-3\n",
    "#train: the rest\n",
    "filesForTesting = {\n",
    "    'bending1': ['dataset1.csv', 'dataset2.csv'],\n",
    "    'bending2': ['dataset1.csv', 'dataset2.csv'],\n",
    "    'cycling': ['dataset1.csv', 'dataset2.csv', 'dataset3.csv'],\n",
    "    'lying': ['dataset1.csv', 'dataset2.csv', 'dataset3.csv'],\n",
    "    'sitting': ['dataset1.csv', 'dataset2.csv', 'dataset3.csv'],\n",
    "    'standing': ['dataset1.csv', 'dataset2.csv', 'dataset3.csv'],\n",
    "    'walking': ['dataset1.csv', 'dataset2.csv', 'dataset3.csv']\n",
    "}\n",
    "testData = []\n",
    "trainData =[]\n",
    "\n",
    "#loops through all the activites and loads them to testing data\n",
    "for i in activity:\n",
    "    path = os.path.join(pathToData, i)\n",
    "    if i in ['bending1', 'bending2']:\n",
    "        files_for_test = filesForTesting[i]\n",
    "    else:\n",
    "        files_for_test = filesForTesting[i]\n",
    "\n",
    "    for file_name in os.listdir(path):\n",
    "        file_path = os.path.join(path, file_name)\n",
    "        if file_name in files_for_test:\n",
    "            # Load test data and add a label\n",
    "            df = pd.read_csv(file_path, skiprows=4)\n",
    "            testData.append(df)\n",
    "        else:\n",
    "            #this specific dataset has wrong delim, replace when it gets to ds4 in bending\n",
    "            if i == 'bending2' and file_name == 'dataset4.csv':\n",
    "                trainData.append(df4)\n",
    "                \n",
    "            # Load training data and add a label\n",
    "            else:\n",
    "                df = pd.read_csv(file_path, skiprows=4, skipfooter=1, engine='python')\n",
    "                trainData.append(df)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   # Columns: time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  \\\n",
      "0                0      42.00       0.00      18.50       0.50      12.00   \n",
      "1              250      42.00       0.00      18.00       0.00      11.33   \n",
      "2              500      42.75       0.43      16.75       1.79      18.25   \n",
      "\n",
      "   var_rss23  \n",
      "0       0.00  \n",
      "1       0.94  \n",
      "2       0.43  \n",
      "   # Columns: time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  \\\n",
      "0                0      41.25       1.30      24.00       0.00      32.33   \n",
      "1              250      41.00       1.00      22.50       1.12      33.50   \n",
      "2              500      44.25       0.83      21.75       0.83      31.50   \n",
      "\n",
      "   var_rss23  \n",
      "0       0.94  \n",
      "1       0.87  \n",
      "2       1.50  \n",
      "   # Columns: time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  \\\n",
      "0                0      47.33       0.47       28.0       0.82      34.25   \n",
      "1              250      47.50       0.50       29.0       1.22      36.00   \n",
      "2              500      47.50       0.50       30.0       0.00      36.75   \n",
      "\n",
      "   var_rss23  \n",
      "0       1.30  \n",
      "1       0.00  \n",
      "2       0.83  \n",
      "   # Columns: time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  \\\n",
      "0                0      44.33       0.47      23.75       0.43      30.00   \n",
      "1              250      44.50       0.50      24.00       0.00      30.25   \n",
      "2              500      44.00       0.71      24.00       0.00      30.50   \n",
      "\n",
      "   var_rss23  \n",
      "0       0.00  \n",
      "1       0.43  \n",
      "2       0.50  \n",
      "   # Columns: time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  \\\n",
      "0                0       42.0       0.71      21.25       0.43      30.00   \n",
      "1              250       41.5       0.50      20.25       1.48      31.25   \n",
      "2              500       41.5       0.50      14.25       1.92      33.00   \n",
      "\n",
      "   var_rss23  \n",
      "0       0.00  \n",
      "1       1.09  \n",
      "2       0.00  \n",
      "   # Columns: time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  \\\n",
      "0                0       34.5        1.5       8.00       1.58       28.0   \n",
      "1              250       36.0        0.0      11.25       3.49       28.5   \n",
      "2              500       36.0        0.0      20.00       1.22       27.0   \n",
      "\n",
      "   var_rss23  \n",
      "0       0.71  \n",
      "1       0.50  \n",
      "2       0.71  \n",
      "   # Columns: time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  \\\n",
      "0                0      32.50       0.50        0.0        0.0       19.0   \n",
      "1              250      32.50       0.50        0.0        0.0       18.5   \n",
      "2              500      32.75       0.43        1.0        0.0       18.0   \n",
      "\n",
      "   var_rss23  \n",
      "0        1.0  \n",
      "1        0.5  \n",
      "2        0.0  \n",
      "   # Columns: time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  \\\n",
      "0                0      28.00       0.00        0.0        0.0      17.25   \n",
      "1              250      29.00       0.00        0.0        0.0      17.33   \n",
      "2              500      28.67       0.47        0.0        0.0      17.00   \n",
      "\n",
      "   var_rss23  \n",
      "0       0.83  \n",
      "1       0.94  \n",
      "2       0.82  \n",
      "   # Columns: time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  \\\n",
      "0                0       24.5       0.50      15.50       0.87       26.0   \n",
      "1              250       25.0       0.00      13.50       3.64       33.5   \n",
      "2              500       26.0       0.71      14.25       3.90       18.5   \n",
      "\n",
      "   var_rss23  \n",
      "0       2.74  \n",
      "1       2.60  \n",
      "2      13.61  \n",
      "   # Columns: time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  \\\n",
      "0                0       32.5       3.50       9.75       2.28      18.50   \n",
      "1              250       39.0       0.82      11.33       4.92      16.50   \n",
      "2              500       42.5       0.50      17.75       2.68      19.67   \n",
      "\n",
      "   var_rss23  \n",
      "0       1.66  \n",
      "1       0.50  \n",
      "2       3.09  \n"
     ]
    }
   ],
   "source": [
    "for df in trainData[:10]:\n",
    "    print(df.head(3)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "19\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(trainData))\n",
    "print(len(testData))\n",
    "print(type(testData)) \n",
    "print(type(trainData))\n",
    "#there are 88 instances in list and df in each  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### i. Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum: Lowest value\n",
    "Maximum: Highest Value\n",
    "Mean: Average value\n",
    "Median: Middle value\n",
    "Mode: Most frequent value\n",
    "Range: Difference between lowest and highest value\n",
    "Standard Deviation: Variation with avalues \n",
    "Q1-Q4: Every 25 percentile of data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ii. Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created function since we are doing it multiple times to all data\n",
    "def featureExtraction(data):\n",
    "    features = {\n",
    "        'min': data.min(),\n",
    "        'max': data.max(),\n",
    "        'mean': data.mean(),\n",
    "        'median': data.median(),\n",
    "        'std': data.std(),\n",
    "        'Q1': data.quantile(0.25),\n",
    "        'Q3': data.quantile(0.75),\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1.min  1.max     1.mean  1.median     1.std   1.Q1   1.Q3  2.min  2.max  \\\n",
      "0  36.25  48.00  43.968017     44.50  1.619873  43.29  44.67    0.0   1.50   \n",
      "1  37.00  48.00  43.459040     43.25  1.384657  42.50  45.00    0.0   1.58   \n",
      "2  33.00  47.75  42.177578     43.50  3.674177  39.10  45.00    0.0   3.00   \n",
      "3  33.00  45.75  41.678434     41.75  2.245821  41.33  42.75    0.0   2.83   \n",
      "4  35.00  47.40  43.947620     44.33  1.553152  43.00  45.00    0.0   1.70   \n",
      "\n",
      "     2.mean  ...     5.std    5.Q1   5.Q3  6.min  6.max    6.mean  6.median  \\\n",
      "0  0.412944  ...  3.321655  20.500  23.75    0.0   2.96  0.555428      0.49   \n",
      "1  0.377829  ...  2.448733  22.250  24.00    0.0   5.26  0.678518      0.50   \n",
      "2  0.696597  ...  3.845985  30.500  36.33    0.0   2.18  0.613904      0.50   \n",
      "3  0.535282  ...  2.413538  28.415  31.25    0.0   1.79  0.384092      0.43   \n",
      "4  0.425407  ...  2.001635  35.450  36.50    0.0   1.79  0.491608      0.43   \n",
      "\n",
      "      6.std  6.Q1  6.Q3  \n",
      "0  0.488330  0.00  0.83  \n",
      "1  0.622693  0.43  0.87  \n",
      "2  0.524798  0.00  1.00  \n",
      "3  0.389176  0.00  0.50  \n",
      "4  0.512714  0.00  0.94  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "#chatgpt:  I have 2 list, trainData and testData, inside the list are dataframes. \n",
    "# Each data frame has 7 columns but i only want the last 6 columns. There are a total of 88 data frames. How do i combine everything\n",
    "dataTableFeatures = []\n",
    "\n",
    "# Process each DataFrame in trainData and testData\n",
    "for df_list in [trainData, testData]:\n",
    "    for df in df_list:\n",
    "        # Extract the last 6 columns with the features\n",
    "        last_six_columns = df.iloc[:, -6:]\n",
    "        # Initialize a dictionary to hold the aggregated features for the last 6 columns\n",
    "        aggregated_features = {}\n",
    "        # Calculate the desired statistics for each of the last 6 columns\n",
    "        #rename for the corresponding time series\n",
    "        last_six_columns.columns = [str(i) for i in range(1, 7)]\n",
    "\n",
    "        for column in last_six_columns.columns:\n",
    "            aggregated_features[column] = featureExtraction(last_six_columns[column])\n",
    "\n",
    "        # Append to table\n",
    "        dataTableFeatures.append(aggregated_features)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "final_table = pd.DataFrame(dataTableFeatures)\n",
    "#flatten for visability, object to float :D\n",
    "final_table_flat = pd.json_normalize(final_table.to_dict(orient='records'))\n",
    "\n",
    "print(final_table_flat.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### iii. Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviations:\n",
      " 1.min       9.569975\n",
      "1.max       4.394362\n",
      "1.mean      5.336482\n",
      "1.median    5.439770\n",
      "1.std       1.772671\n",
      "1.Q1        6.155771\n",
      "1.Q3        5.136647\n",
      "2.min       0.000000\n",
      "2.max       5.062729\n",
      "2.mean      1.574291\n",
      "2.median    1.412978\n",
      "2.std       0.884215\n",
      "2.Q1        0.947312\n",
      "2.Q3        2.126178\n",
      "3.min       2.957401\n",
      "3.max       4.866932\n",
      "3.mean      4.007810\n",
      "3.median    4.035277\n",
      "3.std       0.945411\n",
      "3.Q1        4.220409\n",
      "3.Q3        4.172617\n",
      "4.min       0.000000\n",
      "4.max       2.183625\n",
      "4.mean      1.166265\n",
      "4.median    1.146183\n",
      "4.std       0.458119\n",
      "4.Q1        0.842470\n",
      "4.Q3        1.554079\n",
      "5.min       6.124001\n",
      "5.max       5.747975\n",
      "5.mean      5.677393\n",
      "5.median    5.813650\n",
      "5.std       1.024759\n",
      "5.Q1        6.102394\n",
      "5.Q3        5.530602\n",
      "6.min       0.045838\n",
      "6.max       2.518921\n",
      "6.mean      1.154662\n",
      "6.median    1.086278\n",
      "6.std       0.517810\n",
      "6.Q1        0.758745\n",
      "6.Q3        1.524198\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "sd = final_table_flat.std()\n",
    "print(\"Standard Deviations:\\n\", sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          90% CI Lower  90% CI Upper\n",
      "1.min         8.310739     10.754129\n",
      "1.max         3.289622      5.354982\n",
      "1.mean        4.710695      5.890135\n",
      "1.median      4.775032      5.995268\n",
      "1.std         1.582482      1.947710\n",
      "1.Q1          5.515212      6.657211\n",
      "1.Q3          4.284033      5.842541\n",
      "2.min         0.000000      0.000000\n",
      "2.max         4.627037      5.388936\n",
      "2.mean        1.402511      1.712691\n",
      "2.median      1.240491      1.546097\n",
      "2.std         0.809166      0.942633\n",
      "2.Q1          0.828208      1.033642\n",
      "2.Q3          1.894882      2.284588\n",
      "3.min         2.756017      3.107466\n",
      "3.max         4.121913      5.429660\n",
      "3.mean        3.449058      4.488551\n",
      "3.median      3.440959      4.537314\n",
      "3.std         0.758923      1.121646\n",
      "3.Q1          3.654284      4.660052\n",
      "3.Q3          3.560306      4.735504\n",
      "4.min         0.000000      0.000000\n",
      "4.max         1.978987      2.353867\n",
      "4.mean        1.083419      1.224262\n",
      "4.median      1.053225      1.201208\n",
      "4.std         0.420512      0.485166\n",
      "4.Q1          0.771505      0.889838\n",
      "4.Q3          1.431694      1.623243\n",
      "5.min         4.525535      7.531550\n",
      "5.max         4.805902      6.585610\n",
      "5.mean        4.364841      6.749607\n",
      "5.median      4.519052      6.890939\n",
      "5.std         0.802696      1.223349\n",
      "5.Q1          4.830336      7.186404\n",
      "5.Q3          4.316058      6.546085\n",
      "6.min         0.000000      0.078476\n",
      "6.max         2.251751      2.769868\n",
      "6.mean        1.065639      1.212944\n",
      "6.median      0.989308      1.146806\n",
      "6.std         0.480048      0.545684\n",
      "6.Q1          0.683832      0.807322\n",
      "6.Q3          1.397288      1.601159\n"
     ]
    }
   ],
   "source": [
    "#Chatgpt: how to make a bootstrap function with numpy \n",
    "def bootstrap_sd(data, n_iterations=1000, ci=0.90):\n",
    "    stds = []\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        sample = np.random.choice(data, size=len(data), replace=True)\n",
    "        stds.append(np.std(sample, ddof=1)) \n",
    "    \n",
    "    # confidence intervals\n",
    "    lower_bound = np.percentile(stds, (1 - ci) / 2 * 100)\n",
    "    upper_bound = np.percentile(stds, (1 + ci) / 2 * 100)\n",
    "    \n",
    "    return np.mean(stds), lower_bound, upper_bound\n",
    "\n",
    "#store results in \n",
    "results = pd.DataFrame(columns=['90% CI Lower', '90% CI Upper'])\n",
    "# Iterate over each relevant column in final_table_flat\n",
    "for column in final_table_flat.columns:\n",
    "        # Drop NaN values before bootstrapping\n",
    "        data = final_table_flat[column].dropna().values\n",
    "        mean_sd, lower_ci, upper_ci = bootstrap_sd(data)\n",
    "        results.loc[column] = {\n",
    "            '90% CI Lower': lower_ci,\n",
    "            '90% CI Upper': upper_ci\n",
    "        }\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### iv. Select Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature I select are mean, median, and standard deviation. I want the mean to see the average tendenacy, the median to count out any outliers, and the SD to check for variability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ISLR 3.7.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Linear Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I expected the RSS for the cubic regression model to be lower than the RSS for the Lr model because a cubic model is able to capture higher complexity and in turn, fit the training data better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Linear Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing data, I expect the linear regression model to be lower than  the cubic regression model. Cubic model may be able to capture more complexity, however, it may also capture more noise with a test data set. In this case, a simple model may be better to generalize the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Not Linear Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I expect the RSS to be lower for the cubic model because it can fit the training data more closely even without a relationship due to its flexibility. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Not Linear Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not enough information to tell because performance of both models well depend on any relationships that the data has and how the models will adapt or generalize to them. A cubic model may be able to fit better if the relationship depending on how non linear it is , however, it may also fit to noise thus having a worse performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ISLR 3.7.3 - Extra Practice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ISLR 3.7.5 - Extra Practice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294.435px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "3c20c2d94d2527936fe0f3a300eb11db30fed84423423838e2f93b74eb7aaebc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
